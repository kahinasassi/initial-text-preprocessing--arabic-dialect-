{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dsaraby.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1OvnnZRFDwFRQ1rR9dHtaAKoRz3oab9qx",
      "authorship_tag": "ABX9TyPdU5M97o6mZu0t6eQTvw3r",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kahinasassi/initial-text-preprocessing--arabic-dialect-/blob/master/dsaraby.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# translateration using DSARABY LABERARY"
      ],
      "metadata": {
        "id": "bovEXef08aQ9"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ghWjMv4Xiyq"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk import re\n",
        "from nltk import defaultdict\n",
        "import io\n",
        "from nltk import  word_tokenize\n",
        "from nltk import  sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import codecs\n",
        "import csv\n",
        "import pickle\n",
        "from gensim.parsing.preprocessing import strip_short\n",
        "\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brwkCJeDLAs6"
      },
      "source": [
        "import json,os,codecs\n",
        "class DSAraby:\n",
        "    def __init__(self):\n",
        "        self.load_mapping()\n",
        "        self.letters_to_ret = set()\n",
        "    \n",
        "    \n",
        "    def load_mapping(self):\n",
        "\n",
        "        dir_path = os.path.dirname(os.path.realpath('__file__'))\n",
        "        self.en_to_ar = json.loads(open('/content/drive/My Drive/translateration/assets/mapping.manual.json', encoding='utf-8').read())\n",
        "        self.NWORDS = {}\n",
        "        word_counts = codecs.open('/content/drive/My Drive/translateration/assets/corpus.txt', encoding='utf-8').read().split(\"\\n\")\n",
        "        for word_count in word_counts:\n",
        "            if word_count:\n",
        "                [word, n] = word_count.split()\n",
        "                if word is None or n is None:\n",
        "                    pass\n",
        "                else:\n",
        "                    self.NWORDS[word] = int(n)\n",
        "\n",
        "    \n",
        "    def transliterate(self,sentence, verbose=False):\n",
        "        words = sentence.split()\n",
        "        ret = []\n",
        "        for word in words:\n",
        "            candidates = list(self.transliterate_word(word))\n",
        "            best_candidates = self.sort_by_frequency(candidates)\n",
        "            if len(best_candidates) > 0:\n",
        "                ret.append(self.sort_by_frequency(candidates)[0])\n",
        "            else:\n",
        "                ret.append(word)\n",
        "        return ' '.join(ret)\n",
        "    \n",
        "    def transliterate_word(self,word):\n",
        "        ret = self.transliterate_letter(word,'',True)\n",
        "        self.letters_to_ret = set()\n",
        "        return ret\n",
        "    \n",
        "    def transliterate_letter(self,letters, word, begin='start'):\n",
        "        if len(letters) == 0:\n",
        "            self.letters_to_ret.add(word)\n",
        "            return\n",
        "        \n",
        "        if begin == 'start':\n",
        "            table = self.en_to_ar['start']\n",
        "        elif begin == 'other':\n",
        "            table = self.en_to_ar['other']\n",
        "        else :\n",
        "            table = self.en_to_ar['end']\n",
        "        max_key_len = len(max(list(table), key=len))\n",
        "        for i in range(1, max_key_len + 1):\n",
        "            l = letters[:i]\n",
        "            if l in table:\n",
        "                for ar in table[l]:\n",
        "                    self.transliterate_letter(letters[i:], word + ar,'start')\n",
        "        \n",
        "        return self.letters_to_ret\n",
        "    \n",
        "    def sort_by_frequency(self,candidates): \n",
        "        return sorted(candidates, key=lambda k: self.NWORDS.get(k,0), reverse=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dx7QtTZUVoKE"
      },
      "source": [
        "p1 =  DSAraby()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EH6Z4PwidxJv",
        "outputId": "b5bebc47-0512-4a6f-f8a5-1f1075676b9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "p1.transliterate(\" bsh smahli nta zwiin nchlh b rabi saha\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'بسه صمىحلى نت زوين نشله ب ربي صاح'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSg7LXfOM0MJ",
        "outputId": "d32eb251-630a-4127-821a-a558d7cf6981",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "p1.transliterate(\" sbah l khir alikom \")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'صباح ل خير اليكم'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgEiTEPOdpvl",
        "outputId": "38818f6a-9a72-4388-be8f-f46e2dedc965",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "p1.transliterate(\" wash rak tdir moh rak mlih\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'وش ريك تدير مه ريك ملح'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYShKlp6d0eH",
        "outputId": "151d383a-5ffa-40ad-a89e-b4ae69f76fb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "p1.transliterate(\" bsh smahli \")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'بسه صمىحلى'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96c5iT9mYKgf"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Retn0-lYgqu4"
      },
      "source": [
        "donnee=defaultdict()\n",
        "data=open_pickle(\"/content/drive/My Drive/PROJET FIN ETUDE 2019 2020/ressources/donnee_phonetics.pickle\")#charger dataset après phonétique grouping\n",
        "donnee=defaultdict()\n",
        "#mettre le corpus dans un dictionnaire pour facilier la manipulation \n",
        "for indice in list(data.index.values):\n",
        "  donnee[indice]={'text':data['text'][indice],'label':data['label'][indice]}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qU-hS8brXQt5"
      },
      "source": [
        "cpt=0\n",
        "for indice in donnee.index :\n",
        "        cpt=cpt+1\n",
        "        liste=[]\n",
        "        text=donnee[indice]['text']\n",
        "        for word in tokenization(text):\n",
        "          if(is_arabizi(word) == 1):\n",
        "            print(word)\n",
        "            tr=p1.transliterate(word)\n",
        "            liste.append(tr)\n",
        "          else: \n",
        "            liste.append(word)\n",
        "        print(cpt,\" --- \",indice,\" ::   ==> \",liste)\n",
        "        donnee[indice]={'text': liste, 'label' : donnee[indice]['label']}\n",
        "             "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}