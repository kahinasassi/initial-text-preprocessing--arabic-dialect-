{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "traitement_de_la_langue_arabe.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kahinasassi/initial-text-preprocessing--arabic-dialect-/blob/master/traitement_de_la_langue_arabe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnNrU0dmJTSr",
        "outputId": "b6c72730-acac-4cdc-b81f-03d1f805fb63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk import re\n",
        "from nltk import defaultdict\n",
        "import io\n",
        "from nltk import  word_tokenize\n",
        "from nltk import  sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import codecs\n",
        "import csv\n",
        "import pickle\n",
        "from gensim.parsing.preprocessing import strip_short\n",
        "\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRQRLfTsJYv9"
      },
      "source": [
        "#vérifie si l'encodage est utf_8\n",
        "def check_utf8(filename): \n",
        "        try:\n",
        "            f = codecs.open(filename, encoding='utf-8', errors='strict')\n",
        "            for line in f:\n",
        "                pass\n",
        "            return 1 #valid utf_8\n",
        "        except UnicodeDecodeError:\n",
        "            return 0 #invalid utf_8\n",
        "\n",
        "# ouvrir un fichier text\n",
        "def open_txt_file(chemin):\n",
        "    #open text file atfter checking if its utf_8  encoding    \n",
        "    check=check_utf8(chemin)\n",
        "    if(check == 1):\n",
        "        f = io.open(chemin,'r',encoding='utf8')\n",
        "    elif(check == 0):\n",
        "        f = io.open(chemin,'r')\n",
        "    return f.read()\n",
        "\n",
        "# convertir un text vers une liste\n",
        "def text_to_list(chemin):\n",
        "    text=open_txt_file(chemin)\n",
        "    liste=[]\n",
        "    liste=text.split(\"\\n\")\n",
        "    return liste\n",
        "\n",
        "# ouvrir un fichier pickle\n",
        "def open_pickle(path):\n",
        "  with open(path, 'rb') as handle: \n",
        "    doc= pickle.load(handle)\n",
        "    return doc\n",
        "\n",
        "def save_pickle(variable,filename):\n",
        "    with open(filename, 'wb') as handle:\n",
        "         pickle.dump(variable, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBOPDOPKK39l",
        "outputId": "b63f8c8b-1adf-4a44-9341-9dd3fff6f27e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "pip install unidecode"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
            "\r\u001b[K     |█▍                              | 10kB 17.1MB/s eta 0:00:01\r\u001b[K     |██▊                             | 20kB 1.6MB/s eta 0:00:01\r\u001b[K     |████▏                           | 30kB 2.2MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 40kB 2.5MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 51kB 1.9MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 61kB 2.2MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 71kB 2.4MB/s eta 0:00:01\r\u001b[K     |███████████                     | 81kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 92kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 102kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 112kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 122kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 133kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 143kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 153kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 163kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 174kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 184kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 194kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 204kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 215kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 225kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 235kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 245kB 2.7MB/s \n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rZfpjrCJaYA",
        "outputId": "3e7a96eb-9227-49fe-c3f0-a2f2f766406b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sE1lZNwFLTke"
      },
      "source": [
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer,TfidfTransformer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "def naive_bayes(data):\n",
        "  # Set up training and test sets by choosing random samples from classes\n",
        "  X_train, X_test, y_train, y_test = train_test_split(data['text'].values, data['label'].values, test_size=0.20, random_state=0)\n",
        "  model = MultinomialNB()\n",
        "  vect = CountVectorizer()\n",
        "  vect.fit_transform(data['text'].values)  #train the vectorizer, build the vocablury\n",
        "  tfidf_transformer=TfidfTransformer()\n",
        "  X_train_tfidf=vect.transform(X_train)\n",
        "  X_test_vect=vect.transform(X_test)\n",
        "  model.fit(X_train_tfidf,y_train)\n",
        "  model.score(X_train_tfidf,y_train)\n",
        "  y_pred = model.predict(X_test_vect)\n",
        "  y_pred\n",
        "  from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
        "  print(\"Accuracy: {:.2f}%\".format(accuracy_score(y_test, y_pred) * 100))\n",
        "  print(\"\\nCOnfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhUn5fYIJlaB"
      },
      "source": [
        "#Evaluation de la traduction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42KsGKzmJciT",
        "outputId": "93f18d46-ee5c-4aeb-8578-0799cec6f12e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "source": [
        "data=open_pickle(\"/content/drive/My Drive/PROJET FIN ETUDE 2019 2020/ressources/data_traduit.pickle\")\n",
        "for indice  in list(data.index.values): \n",
        "    text=str(data['text'][indice])\n",
        "    data['text'][indice]=text\n",
        "naive_bayes(data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 68.61%\n",
            "\n",
            "COnfusion Matrix:\n",
            " [[ 817  140  283]\n",
            " [ 240  744  271]\n",
            " [ 181  123 1145]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uoe8miz9Jszb"
      },
      "source": [
        "#Evaluation du groupement phonitique"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qr8OolDOJxMJ",
        "outputId": "84ac391c-bdc9-4b77-e4a1-aef45d24ad44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "source": [
        "data=open_pickle(\"/content/drive/My Drive/PROJET FIN ETUDE 2019 2020/ressources/donnee_phonetics.pickle\")\n",
        "for indice  in list(data.index.values): \n",
        "    text=str(data['text'][indice])\n",
        "    data['text'][indice]=text\n",
        "naive_bayes(data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 70.51%\n",
            "\n",
            "COnfusion Matrix:\n",
            " [[ 838  119  305]\n",
            " [ 199  739  328]\n",
            " [ 171  112 1373]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zosQEmi1J-Xb"
      },
      "source": [
        "#Evaluation de la translatération"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WC7RH62LKBlY",
        "outputId": "4e3299bf-304d-436f-8630-974468903e6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "source": [
        "data=open_pickle(\"/content/drive/My Drive/PROJET FIN ETUDE 2019 2020/ressources/donne_yamli.pickle\")\n",
        "for indice  in list(data.index.values): \n",
        "    text=str(data['text'][indice])\n",
        "    data['text'][indice]=text\n",
        "naive_bayes(data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 69.77%\n",
            "\n",
            "COnfusion Matrix:\n",
            " [[ 831  151  256]\n",
            " [ 229  777  303]\n",
            " [ 188  138 1311]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhYxLMDLKYbp"
      },
      "source": [
        "#Traitement de la langue arabe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDz6ArmgKbdt"
      },
      "source": [
        "url=\"/content/drive/My Drive/PROJET FIN ETUDE 2019 2020/ressources/stopwrd.txt\"\n",
        "stopwords= text_to_list(url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bq-5lCF5KiHu"
      },
      "source": [
        "# traitement de la langue arabe\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTDvF1HlKkSk"
      },
      "source": [
        "import re\n",
        "from unidecode import unidecode\n",
        "\n",
        "def tokenization(text):\n",
        "    return set(word_tokenize(text)) \n",
        "\n",
        "def is_arabizi(text):\n",
        "    html = len(re.findall(r'[A-Za-z0-9]+',text))\n",
        "    if(html>0):\n",
        "      text = unidecode(text)\n",
        "      return text.replace(text, f' ')\n",
        "    else:\n",
        "      return text\n",
        "\n",
        "def remove_fr(texte):\n",
        "   word= tokenization(texte) \n",
        "   x=str(texte)\n",
        "   for val in word:\n",
        "     tt=is_arabizi(val)\n",
        "     x = x.replace(val, tt)\n",
        "   return x \n",
        "def remove_ponctuation(text):\n",
        "   puncts = [',', '.', '\"', ':', ')', '(', '-', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '£', \n",
        " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
        " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', '،','؟','/',\n",
        " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
        " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗',' ️�','▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
        "   \n",
        "   x = str(text)\n",
        "   for punct in puncts:\n",
        "      if punct in x:\n",
        "          x = x.replace(punct, f' ')\n",
        "   return x\n",
        "arabic_diacritics = re.compile(\"\"\"\n",
        "                             ّ    | # Tashdid\n",
        "                             َ    | # Fatha\n",
        "                             ً    | # Tanwin Fath\n",
        "                             ُ    | # Damma\n",
        "                             ٌ    | # Tanwin Damm\n",
        "                             ِ    | # Kasra\n",
        "                             ٍ    | # Tanwin Kasr\n",
        "                             ْ    | # Sukun\n",
        "                             ـ     # Tatwil/Kashida\n",
        "                         \"\"\", re.VERBOSE)\n",
        "\n",
        "def remove_diacritics(text):\n",
        "    text = re.sub(arabic_diacritics, '', text)\n",
        "    return text\n",
        "\n",
        "def remove_arabic_ponctuation(text):\n",
        "   puncts = ['﴿', '﴾', '۩‎', '۞', '۝', '(', '-', '۔‎', '٭', \"؟‎\", '؞', '؛‎', '؏', '؎', '؍', '،', '؆', '؇', '؈', '؄', '؁', '؀']\n",
        "   x = str(text)\n",
        "   for punct in puncts:\n",
        "      if punct in x:\n",
        "          x = x.replace(punct, f' ')\n",
        "   return x\n",
        "\n",
        "def normalize_arabic(text):\n",
        "    text = re.sub(\"[إأآا]\", \"ا\", text)\n",
        "    text = re.sub(\"ي\", \"ى\", text)\n",
        "    text = re.sub(\"ؤ\", \"ء\", text)\n",
        "    text = re.sub(\"ئ\", \"ء\", text)\n",
        "    text = re.sub(\"ة\", \"ه\", text)\n",
        "    text = re.sub(\"گ\", \"ك\", text)\n",
        "    return text  \n",
        "\n",
        "def remove_stop_wrd(texte):\n",
        "  word= tokenization(texte) \n",
        "  x=str(texte)\n",
        "  ma_lis=[]\n",
        "  for val in word:\n",
        "    for text  in  stopwords: \n",
        "      if(val == text):\n",
        "        x = x.replace(val,'')  \n",
        "  return x  \n",
        "all_dz_msa_stopwords=open_pickle(\"/content/drive/My Drive/PROJET FIN ETUDE 2019 2020/ressources/all_dz_msa_stopwords.pickle\")\n",
        "def remove_stop_words(texte):\n",
        "\n",
        "  word= tokenization(texte) \n",
        "  x=str(texte)\n",
        "  ma_lis=[]\n",
        "  for val in word:\n",
        "    for text  in  all_dz_msa_stopwords: \n",
        "      if(val == text):\n",
        "        x = x.replace(val,'')  \n",
        "  return x  \n",
        "def remove_short(text):\n",
        "  lower_word=text.lower()\n",
        "  liste=tokenization(lower_word)\n",
        "  sentence=''\n",
        "  for word in liste:\n",
        "    if(len(word) == 1):\n",
        "        new=''\n",
        "        sentence=sentence+''+new\n",
        "    else:\n",
        "      if(sentence == ''):\n",
        "        new=word\n",
        "        sentence=sentence+''+new\n",
        "      else:\n",
        "        new=word\n",
        "        sentence=sentence+' '+new\n",
        "      \n",
        "  return sentence\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0h_coJAOKwiq",
        "outputId": "4bc34126-e60f-42b1-ee0f-d12967bc7964",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "data[\"text\"] = data.text.map(lambda x: remove_short(x)) # suppression des mots court --------------------- Accuracy: 67.97%\n",
        "naive_bayes(data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 70.22%\n",
            "\n",
            "COnfusion Matrix:\n",
            " [[ 831  141  266]\n",
            " [ 222  782  305]\n",
            " [ 184  128 1325]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUvEVKehLtkM",
        "outputId": "15838167-1ca8-4143-ed06-3812b111644a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "'''data[\"text\"] = data.text.map(lambda x: remove_fr(x)) # suppression des mots fr_arabe -----------------------Accuracy: 68.35%\n",
        "data[\"text\"] = data.text.map(lambda x: remove_fr(x))# on laisse tous Accuracy: 68.11%\n",
        "naive_bayes(data)'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'data[\"text\"] = data.text.map(lambda x: remove_fr(x)) # suppression des mots fr_arabe -----------------------Accuracy: 68.35%\\ndata[\"text\"] = data.text.map(lambda x: remove_fr(x))# on laisse tous Accuracy: 68.11%\\nnaive_bayes(data)'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtXFz7Kqk5oS"
      },
      "source": [
        "# remove_diacritics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6f5TsR_LLzbi",
        "outputId": "fe117218-8fbe-47c4-b3f9-430dfa0b7303",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "data[\"text\"] = data.text.map(lambda x: remove_diacritics(x))\n",
        "naive_bayes(data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 70.29%\n",
            "\n",
            "COnfusion Matrix:\n",
            " [[ 831  140  267]\n",
            " [ 220  785  304]\n",
            " [ 182  130 1325]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uN79Xdvvk95b"
      },
      "source": [
        "# remove_arabic_ponctuation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLYjSsvbL5f9",
        "outputId": "3dd1b04e-6a78-4cde-9f2a-7549f8bf28f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "data[\"text\"] = data.text.map(lambda x: remove_arabic_ponctuation(x))\n",
        "naive_bayes(data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 70.29%\n",
            "\n",
            "COnfusion Matrix:\n",
            " [[ 831  140  267]\n",
            " [ 220  785  304]\n",
            " [ 182  130 1325]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPmWYb_UlAMN"
      },
      "source": [
        "# normalize_arabic"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GObNIJ_vL-st",
        "outputId": "df16ed25-972d-44d9-adfe-5d48fed8aa26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "data[\"text\"] = data.text.map(lambda x: normalize_arabic(x))\n",
        "naive_bayes(data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 70.34%\n",
            "\n",
            "COnfusion Matrix:\n",
            " [[ 828  142  268]\n",
            " [ 221  785  303]\n",
            " [ 175  132 1330]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7-x6qoFlCk_"
      },
      "source": [
        "# remove_stop_words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfKUBXkDMDCQ",
        "outputId": "1c91bc0f-b3a8-4537-e8e0-9fcc5be3880c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "data[\"text\"] = data.text.map(lambda x: remove_stop_words(x))\n",
        "naive_bayes(data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 69.72%\n",
            "\n",
            "COnfusion Matrix:\n",
            " [[ 813  152  273]\n",
            " [ 214  766  329]\n",
            " [ 162  137 1338]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PHTUqaDMJHA"
      },
      "source": [
        "data=open_pickle(\"/content/drive/My Drive/PROJET FIN ETUDE 2019 2020/ressources/finale_data.pickle\")\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}